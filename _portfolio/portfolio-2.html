---
title: "2. Probabilistic binary latent variable model infers hidden structure in spike trains."
excerpt: "<img src='/images/CA_model.png' width='350' height='300'/> <img src='/images/CA_Robustness.png' width='550' height='400'/><br/> We introduce a novel, probabilistic binary latent variable (BLV) model to detect noisy or approximate repeats of patterns in sparse binary data. We adapt the framework of the 'Noisy-OR model', previously used to infer hidden diseases from observed symptoms, to neuron spiking mechanics and demonstrate the model's capability by extracting structure in recordings from retinal neurons. There, the task is to 'explain' spikes of individual neurons in terms of groups of neurons, 'Cell Assemblies' (CAs), that often fire together, due to mutual interactions or other causes. The conditional probability kernels of the latent components are learned from the data in an expectation maximization scheme, alternately performing inference of latent states and parameter adjustments to the model. We thoroughly validate the model on synthesized spike trains constructed to statistically resemble recorded retinal responses to white noise stimulus and natural movie stimulus in data. Finally, we apply our model to spiking responses recorded in retinal ganglion cells (RGCs) during stimulation with a movie and discuss the found structure."
collection: portfolio
---

<style>
  body {
    margin: 0;
  }

  figure {
    width: 31%; /* Adjusted to leave space for boundaries */
    margin: 0.5 0.5%; /* Added margin for space between figures */
    padding: 10px;
  }

  video,
  img {
    width: 100%;
    display: block;
  }
</style>


<p style="text-align: center; font-size:11pt"><strong>Skills & Expertise:</strong> Latent Variable Model, Bayesian probability, Expectation Maximization, Synthetic Data, Model Validation, Exploratory Data Analysis </p>

<br/><header style="text-align: center">The Data</header><br/>

<!--  Embed video of white noise stim and natural movie stim with PSTH and RFs. -->
<div style="display: flex; width: 100%;">

  <figure style="width: 31%;">
    <video controls width="100%" autoplay loop muted >
      <source src="/images/movie_wnrep.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
    <figcaption><strong>a)</strong>  White Noise Stimulus</figcaption>
  </figure>

  <figure style="width: 31%;">
    <img src="/images/CA_RFs.jpg" alt="Image" style="width: 100%;">
    <figcaption><strong>b)</strong> Receptive Fields of 55 RGC's</figcaption>
  </figure>

  <figure style="width: 31%;">
    <video controls width="100%" autoplay loop muted >
      <source src="/images/movie_catrep.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
    <figcaption><strong>c)</strong> Natural Movie Stimulus</figcaption>
  </figure>

</div> 

<p style="text-align: center; font-size:10pt"><strong>fig. 1</strong> Stimulus and Retinal Ganglion Cell Receptive Field Lattice </p>


White Noise stimulus, uncorrelated in space and time, is very different from Natural Movie stimulus and the retina responds very differently to the two types of input. Visual inspection of retinal responses to white noise stimulus vs. natural movie stimulus reveal clear spatial and temporal differences. The figure below shows peristimulus time histograms (PSTH) of a population of retinal ganglion cells (RGCs) responding to the two stimulus types. <a href="https://www.nature.com/articles/nature07140.pdf">Generalized Linear Models (GLMs)</a> of retina well capture the responses to white noise, but they fail to capture responses to natural movies, which contain spatial and temporal correlations in the stimulus and responses. Aside from the clear spatial correlations in responses to natural movies, it appears that single cell responses are also more variable (smeared out) in time trial-to-trial and that those temporal variations may be shared across clusters of cells.


  <figure style="width: 100%;">
    <img src="/images/CA_PSTHs.jpg" alt="Image" style="width: 100%;">
    <figcaption><strong>fig. 2</strong> Responses and Receptive Fields of Off-Brisk Transient Retinal Ganglion cells: Left Top, responses to white noise. Left Bottom, responses to natural movie. Color indicates #Trials in which cell (y-axis) spiked during a 1ms interval of stimulus presentation (x-axis). Right, receptive fields for 55 recorded RGCs. Geometric RF relationships in visual space maintained in cell ordering.</figcaption>
  </figure>

<br/><header style="text-align: center">The Model</header><br/>

To explore this phenomenon, we introduce the Binary Latent Variable (BLV) Model, which allows for two causes of a cell's observed firing $\mathbf{y}$, its individual intrinsic firing rate, $\mathbf{R}_i$, and its membership $\mathbf{P}_{ia}$ in an active Cell Assembly $\mathbf{z}$. The model schematic is displayed below.<br/>


<p align="center" width="100%">
  <img src='/images/CA_model.png' align='center' width='400' height='250'/> 
  <p style="text-align: center; font-size:10pt"><strong>fig. 3</strong> Schematic of BLV model applied to detect patterns in recordings of neural activity: In the model, individual spikes in a spike-word $\mathbf{y}$ can arise from two sources. First, each of the N cells has some probability of firing without any latent unit activity, expressed by N-vector $\mathbf{R}$. Second, a cell can fire because it is a member of a latent cell group, which is active. For the M latent variables, the membership structure between latent and observed variables is expressed by the MxN Matrix $\mathbf{P}$ of conditional probabilities. The scalar Q parameter sets a binomial prior on the activity in the sparse vector of latent variables, $\mathbf{z}$. </p>
</p>

 Assuming sparse activation and binomial distribution of latent states and conditional independence of observation vector given latent variables, the model yields the joint probability for a single observed unit's activity $y_i$ and latent vector $\mathbf{z}$ given by (see <a href="https://chris-warner-ii.github.io/publication/2022-paper-3">paper</a> for further model assumptions and full derivation):<br/><br/>

<p align="center" width="100%">
  $p(y_i,\mathbf{z}) = p(\mathbf{z}) p(y_i \vert \mathbf{z})$ <br/><br/>

  $p(y_i,\mathbf{z}) = p(\mathbf{z}) p(y_i=0 \vert \mathbf{z})^{(1-y_i)} p(y_i=1 \vert \mathbf{z})^{y_i}$ <br/><br/>

  $p(y_i,\mathbf{z}) = p(\mathbf{z}) p(y_i=0 \vert \mathbf{z})^{(1-y_i)} \bigg [ 1-p(y_i=0 \vert \mathbf{z}) \bigg ] ^{y_i}$ <br/><br/>

  $p(y_i,\mathbf{z}) = {M\choose |\mathbf{z}|} \cdot  Q^{|\mathbf{z}|}\cdot (1-Q)^{\big( M - |\mathbf{z}| \big)}   \bigg [ R_i^{ \big ( 1 - \frac{|\mathbf{z}|}{M} \big ) } \prod_{a=1}^M (P_{ia})^{z_a} \bigg ]^{(1-y_i)} \bigg [ 1 -  R_i^{ \big ( 1 - \frac{|\mathbf{z}|}{M} \big ) } \prod_{a=1}^M (P_{ia})^{z_a} \bigg ]^{y_i}$ <br/><br/>
</p>

We fit the BLV Model using an Expectation Maximization (EM) framework. For each observed spike-word $\mathbf{y}$, learning proceeds in two steps. First, the latent variables $\mathbf{z}$ are inferred with model parameters fixed. Then, model parameters are adjusted to maximize the derivative of the log joint probability of latent and observed states $p(\mathbf{y},\mathbf{z})$. with respect to each parameter. Derivatives are computed with respect to unconstrained parameters ($q, r, \rho$) and passed through a logistic parametrization, $P = \sigma(\rho)$, to enable the interpretation of constrained parameter values ($Q, R, P$) as probabilities $\in [0,1]$. Derivatives with respect to each model parameter are shown below: <br/><br/>

<p align="center" width="100%">
  $\frac{\partial \: log \: p(y_i,\mathbf{z})}{\partial q} = |\mathbf{z}| -  M \sigma(q)$ <br/><br/>

  $\frac{\partial \: log \: p(y_i,\mathbf{z})}{\partial r_i} = \bigg ( 1-\frac{|\mathbf{z}|}{M} \bigg ) \: \bigg (1-\sigma(r_i) \bigg ) \: \bigg [ (1-y_i) - \frac{y_i \:C_i}{(1-C_i)} \bigg ]$ <br/><br/>

  $\frac{\partial \: log \: p(y_i,\mathbf{z})}{\partial \rho_{ia}} = z_a \: \bigg (1-\sigma(\rho_{ia}) \bigg ) \: \bigg [ (1-y_i) - \frac{y_i \:C_i}{(1-C_i)} \bigg ]$ <br/><br/>
</p>

where $C_i := \sigma(r_i)^{\big ( 1-\frac{|\mathbf{z}|}{M} \big )} \prod_{a=1}^M \sigma(\rho_{ia})^{z_a}$ <br/><br/>


<br/><header style="text-align: center">Model Validation</header><br/>

Before applying the model to real retinal data, it is important to first validate the model using synthetic data to ensure that the model is able to learn known structure that has been embedded into the data. To that end, we constructed synthetic data that  matched certain moments measured in retinal spike train responses to white noise and natural movies, process described in figure 4 below. Interestingly, the difference between model parameters fit to the regimes yield an intuitive interpretation. The best-fit parameters reveal that, under the assumptions of the model, responses to natural movie stimulus contain fewer active cell assemblies in any observed spike-word, while each individual cell assembly contains more cells with stronger membership or participation in that assembly, when compared to responses of the same cell population responding to white noise stimulus.


<p align="center" width="100%">
  <img src='/images/CA_synthetic_data_moments.png' align='center' width='750' height='550'/>
  <p style="text-align: center; font-size:10pt"><strong>fig. 4</strong> Fitting synthetic model to recorded spike-word moments: Comparison of spike-word moments from retinal responses to white noise (Wnz) in green, and natural movie (Mov) in blue to synthetic data fitted to natural movie responses (Syn) in red. Recording data binned at 5ms. Top row from left to right shows probability density functions for spike-word length, |y|, average single-cell activity, ⟨yi⟩, and pairwise cell coactivity, ⟨yi · yj⟩. Bottom row shows quantile-quantile (QQ) plots - a pair of cumulative density function plotted against each other. See legend, left plot. QQ values measure average deviation from the unity line, larger values indicating bigger differences between distributions. </p>
</p>

Multiple models were trained on synthetic data and the structure discovered by each model was compared to what other models learned and to the ground truth. Figure 5 walks through the process of quantifying model match to other models and model match to ground truth. The cosine similarity measure, defined as the angle between two N-dimensional vectors in cell-assembly membership space, was used:  <br/><br/>

<p align="center" width="100%">
  $cs(v_1,v_2)= \frac{(v_1)^\top v_2}{\|v_1\| \|v_2\|}$  <br/><br/>
</p>

From the model validation procedure, we verified that when two models trained on structure resembling natural movie responses learned similar structure, that structure also existed in the ground truth model. This allows us to be reasonably confident that cell assembly structure that is learned robustly across multiple models in real neural data is meaningful.


<div style="display: flex; width: 100%;">

  <figure style="width: 49%;">
    <img src="/images/CA_synth_natMov1.png" alt="Image" style="width: 100%;">
    <figcaption><strong>fig. 5a)</strong> Assessing similarity of cell assembly structure learned by different models trained on synthetic data fitted to natural movie responses. Left column shows M × M matrices of cosine similarity (cs) values between all cell assembly pairs across a model pair. Top, with arbitrary order and bottom, with cell assemblies matched across models based on pairwise cs. Right shows the pair of $\mathbf{P}$ matrices with columns, cell assemblies, aligned to maximize cs across all matched pairs. Blue bars in bottom right show cs for each matching column pair above. Note that blue bars are equivalent to diagonal elements in matched cs matrix in bottom left. The difference between diagonal means of the matched (bottom left) and the unmatched (top left) cs matrices, $\Delta cs$, provides a simple scalar measure of model similarity.</figcaption>
  </figure>

  <figure style="width: 49%;">
    <img src="/images/CA_synth_natMov2.png" alt="Image" style="width: 100%;">
    <figcaption><strong>b)</strong> Cross-validation of six models trained on synthetic data matched fitted to natural movie responses. Each element in matrix shows average $\Delta cs$ (relative to null without CA matching) between all matched CA pairs within a model pair. Note that for visual clarity using this color scheme, the unity diagonal elements have been set to zero. Vector labeled ’GT’ on right shows average $\Delta cs$ metric between model and ground truth model used to generate the data. Example described in left panel, comparison between Mod2 and Mod1B, hilighted with orange square. Each element of matrix contains a similar pairwise model comparison. </figcaption>
  </figure>

</div> 

<!--
<figure style="width: 100%;">
  <img src="/images/CA_synth_natMov3.png" alt="Image" style="width: 100%;">
  <figcaption><strong>fig. 5 c)</strong> Individual cross-validated CAs often match GT: Results for synthetic data fitted to natural movie responses. The left plot compares the cosine similarity (cs) between matched pairs of CAs in two learned models (Mod1 and Mod2) on the x-coordinate to cs between each model’s CA and matching CA in the GT on the y-coordinate. Each point represents similarities between one CA in a model (Mod1 in blue and Mod2 in green) and a matching CA in the GT. The points are connected by a faint grey vertical line if CAs in models are matched to different GT CAs. Red ’o’ highlights CA in model pair with smaller cs match to GT, and a black diamond marks points for which the two models are matched to the same CA in the GT (i.e., where green and blue points coincide). Larger blue and green ’+’ show $\mu$ and $\sigma$ of each model’s CA population. The right plot shows the distribution of distances in the left plot between points and the unity line. Pearson correlation coefficients (r) for three scatter groups in left plots are shown in legend on the right.</figcaption>
</figure>
-->

<br/><header style="text-align: center">Exploratory Data Analysis</header><br/>

In order to systematically and methodically explore the structure found by our BLV model in neural data, we constructed four metrics to quantitatively answer four questions.<br/>

<p style="text-align: left; font-size:11pt">
  <strong>(Q1).</strong>  How many cells participate in a CA? Are the CA’s membership boundaries crisp or diffuse? <br/>
  <strong>(M1). Membership Crispness ($C_M$)</strong>, based on the d′ metric from Signal Detection Theory, quantifies how well member cells and nonmember cells are separated in the corresponding column $P_{.a}$ of the learned model parameters. The conditional activation probabilities of member and nonmember cells are modeled by normal distributions and $C_M$ is computed as: 
</p>

<p align="center" width="100%">
  $C_M = \frac{\mu_{in}-\mu_{out}}{\sqrt(\sigma_{in}^2+\sigma_{out}^2)} $  <br/>
</p>

<p style="text-align: left; font-size:11pt">
  with $\mu_{in}$ and $\sigma_{in}$ the mean and standard deviation of P values of cells determined to be ”in” the CA, the remainder of cells being labeled as ”out”. Three illustrative examples of CAs with varying CM values are shown in figure 6.
</p>


<p align="center" width="100%">
  <img src='/images/CA_Crispness.png' align='center' width='750' height='150'/>
  <p style="text-align: center; font-size:10pt"><strong>fig. 6</strong> Membership Crispness $C_M$ example: Cell assembly $C_M$ values ranging from ”diffuse” (a) to ”crisp” (c). In each panel, numbered ovals represent the cells the receptive fields with the saturation of the red color indicating the degree of CA membership, legend above dashed line. Corresponding column $P_{·a}$ of fitted model parameters shown on the right.</p>
</p>

<p style="text-align: left; font-size:11pt">
  <strong>(Q2).</strong>  Are individual CAs found robustly and reliably across models, with similar spatial membership structure and similar temporal response profiles? <br/>
  <strong>(M2). Cross-validation Robustness ($R_X$)</strong> quantifies how repeatable membership structure and temporal activation of cell assemblies is when learned across training multiple BLV models on different sub-samplings and cross-validation splits of the same data. Figure 7 illustrates how average membership cosine similarity ($cs_M$) and average temporal similarities ($cs_{\tau}$) are computed for a set of corresponding CA's learned across multiple models and how they are combined into a single Robustness measure: 
</p>

<p align="center" width="100%">
  $R_X = \sqrt{( \langle cs_{\tau} \rangle _X . \langle cs_{M} \rangle _X )}$  <br/>
</p>

<p style="text-align:left; font-size:11pt">
  where $\langle cs \rangle _X$ indicates similarity to matching CAs, either membership or temporal, averaged across pairs of fitted BLV models. $R_X$ is bounded between 0 and 1, obtaining large values only when temporal and membership similarity across matching CAs in multiple models are both high.
</p>


<p align="center" width="100%">
  <img src='/images/CA_Robustness.png' align='center' width='750' height='350'/>
  <p style="text-align: center; font-size:10pt"><strong>fig. 7</strong> Cross-validation Robustness $R_X$ example: (a). Top two panels show raster plots time vs. trial for activity in latent variable for CA z50 (red) and in observed variables (spikes) for the member cells of z50, y7 (green), y10 (blue), y31 (black), and y35 (cyan). (b). PSTHs of activity of the CA corresponding to z50 above inferred by six cross-validated models, each normalized with total number of activations of this CA (white numbers on right). Different models are arranged on the y-axis, PSTH from CA z50 shown above is the top line, PSTHs from CAs in other models, matched to z50, shown in other rows. (c). Membership cosine similarity ($cs_M$) vs. temporal cosine similarity ($cs_{\tau}$) for each CA in model with matching CAs, averaged across 5 matching CAs in 5 cross-validation models. (d). Cross-validation Robustness ($R_X$) vs. Membership Crispness ($C_M$) metrics for all CAs in one model. CA z50 highlighted with red ”50” in panels c & d. (e). Cell RFs. Saturation level of red color indicates CA membership strength, $P_{·a}$ value, and outline colors match raster colors in panel a. (f). Matching columns of $\mathbf{P}$ matrices of different models for CA z50, with z50 in the leftmost column and matching CAs of the other cross-validation models in other columns, labels (model, CA id) as in (b).</p>
</p>

<p style="text-align:left; font-size:11pt">
  <strong>(Q3).</strong>  In models trained on multiple cell-types, do CAs cross cell-type boundaries and include interactions between cells of different types? <br/>
  <strong>(M3). Cell-type Heterogeneity ($H$)</strong>, is a measure of how mixed the membership of a cell assembly is across a pair of cell-types. We define heterogeneity as:
</p>

<p align="center" width="100%">
  $H = \frac{min(\#_{ct1},\#_{ct2})}{avg(\#_{ct1},\#_{ct2})}$<br/>
</p>

<p style="text-align: left; font-size:11pt">
  where $\#_{cti}$ is the number of cells of type i participating in the CA. H is bounded between 0 and 1, requiring mixed CA participation to be nonzero, and taking a maximum value of 1 when each cell type contributes half of the cells to the CA. A sample of a cell assembly involving offBT and offBS cells with high heterogeneity value is shown in Fig 8. 
</p>

<p align="center" width="100%">
  <img src='/images/CA_Hetero.png' align='center' width='750' height='150'/>
  <p style="text-align: center; font-size:10pt"><strong>fig. 8</strong> Cell-type Heterogeneity H example: Single CA comprised of offBT (red, left) and onBT (blue, right) cell types. Ovals indicate cell RFs and color intensity indicates membership strength, i.e., $P_{·a}$ value. Legend above dashed line.</p>
</p>


<p style="text-align:left; font-size:11pt">
  <strong>(Q4).</strong>  Are spike-words observed during CA activity significantly different from what the GLM retinal model would predict for the same stimulus?<br/>
  <strong>(M4). Difference from Null Prediction ($\Delta P_y$)</strong>, a measure of the significance of a CA, captures the degree to which observed cell firing associated with a CA differs from predictions of a retinal model that assumes independent firing of the neurons as a null model. The null hypothesis is a Generalized Linear Model (GLM) without cell interactions, in which each cell is only driven by the stimulus in its RF and its own spike-history. The computation of $\Delta P_y$

<p align="center" width="100%">
  $\Delta P_y = 1 - cs_{\tau}(PSTH(z_a),p(\mathbf{y})_{null})$<br/>
</p>

<p style="text-align: left; font-size:11pt">
  illustrated in figure 9 includes a computation of the temporal cosine similarity ($cs_{\tau}$) between null model predictions and predictions from the BLV model. The probability under the null model, $p(\mathbf{y})_{null}$, (green trace) is computed at every point in time based on GLM simulated spike-rates and observed spike-word $\mathbf{y}$, averaged across all spike-words observed while $z_a$ is active in the full dataset. The inference time course of the CA, $PSTH(z_a)$, (red trace) is computed by inferring $\mathbf{z}$ activity for all observed $\mathbf{y}$ in full dataset after model parameters have been fixed and placing each $z_a$ activation in time. Differences between the two traces highlight spike-train structure captured by $z_a$ in the latent variable model which is not explained by rate-coded stimulus correlations.
</p>

<p align="center" width="100%">
  <img src='/images/CA_Del_Py_null.png' align='center' width='750' height='150'/>
  <p style="text-align: center; font-size:10pt"><strong>fig. 9</strong> Difference from Null $\Delta P_y$ example: PSTH of Cell Assembly $z_a$ in red. In green, p(y) under GLM null model for all y observed when $z_a = 1$. In blue, KL-divergence between $p(y_i)_{null}$ and $p(y_i|z_a = 1, z_{\not a} = 0)$, the probability of an observed state under the BLV model with $z_a$ active and all other z’s inactive (not further used).</p>
</p>

<br/><header style="text-align: center">Results in Retina</header><br/>

We applied these methods and metrics to real retinal responses, which lack any ground truth, and discussed the structure that the model was ablew to discover in the different data. The BLV model was trained on spike train data collected from in vitro rat retinal ganglion cells (RGCs) from 55 Off-Brisk Transient (offBT), 39 Off-Brisk Sustained (offBS) and 43 On-Brisk Transient (onBT) cells. The data we analyze consists of RGC spike-train responses to 200 trial repeats of 5 second clips of white noise and natural movie stimulus from the ”Cat Cam” data set. As a control, we also generated data from a GLM model fit to natural movie responses and trained a BLV model on those GLM simulated responses. Each model was trained on between 500k and 1M spike words extracted from raw spike trains binned at 5ms. Observing figure 10, it is clear that models trained on offBT responses to each of the different stimuli have learned very different structure.

<p align="center" width="100%">
  <img src='/images/CA_models_real_data.png' align='center' width='750' height='150'/>
  <p style="text-align: center; font-size:10pt"><strong>fig. 10</strong> Models trained on 55 Off-Brisk Transient responses to different stimuli: $\mathbf{P}$ matrices for models trained on retinal responses to white noise stimulus (a), retinal responses to natural movie stimulus (b) and GLM simulated responses to (same) natural movie stimulus (c). Cell ID on y-axis, CA ID on x-axis. Yellow indicates crisply defined cell membership, i.e., $p(y_i=1|z_a=1) \approx 1$.</p>
</p>

Further, the structure found in a single model trained on natural movie responses (in figure 10) is robustly found across multiple models trained on subsets of the same data (in figure 11). Less structure is found in models trained on white noise responses and GLM simulated responses.

<p align="center" width="100%">
  <img src='/images/CA_cosSim_real_data.png' align='center' width='750' height='150'/>
  <p style="text-align: center; font-size:10pt"><strong>fig. 11</strong> Cross-validation of models trained on recorded retinal responses: Display similar to Fig 5. Similarity of CA membership structure across model pairs for six models trained on white noise retinal responses (a), natural movie responses (b), and GLM simulated responses to same natural movie stimulus (c). Within each panel, matrix off-diagonal elements show average $\Delta cs$ (relative to null without CA matching) between all matched CA pairs within a model pair. Here diagonal value indicates average between a model and all other models, i.e. the average across a row. Numbers on left show average conditional log probability computed on hold out set of half of all spike-words, i.e., cross-validation, for each model. Vector on right shows average change from initialization for all CAs in model, defined as 1 − cs.</p>
</p>





Find the full publication <a href="https://chris-warner-ii.github.io/publication/2022-paper-3">here</a>,

and the code repo <a href="https://github.com/chris-warner-II/Cell_Assembly_Codebase">here</a>.


