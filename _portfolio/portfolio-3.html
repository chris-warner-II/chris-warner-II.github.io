---
title: "ML Protein Design (Professional Work)"
excerpt: "<img src='/images/CODA_Airflow_Dags_flowchart.pdf' width='500' height='400'><img src='/images/CODA_ec50_ligand_and_ach2.png' width='400' height='350'><br/> As Machine Learning Engineering consultant for CODA Biotherapeutics, I contributed to an algorithm that guides development of a chemogenetic platform designed to modulate neuronal activity to therapeutic effect. The model predicts the fitness of novel synthetic ligand-gated ion channels engineered to be selectively responsive to specific small molecules. Models are trained on data from proteins synthesized and measured in the lab, a slow and expensive process. We used machine learning to predict on novel protein-ligand combinations to decrease the time and cost of the discovery process."
collection: portfolio
---

<p style="text-align: center; font-size:11pt"><strong>Skills & Expertise:</strong> Bioinformatics, Multi-layer Perceptron, Keras, LLM Embeddings, AWS Cloud Computing, Weights & Biases, Docker containers, Python </p>


Note: Even though <a href="https://www.biospace.com/article/in-tough-market-bay-area-biotech-coda-shutters/">CODA Biotherapeutics</a> has closed since my employment, I will be selective about what details of the project I share. I am attempting to strike a balance between being clear about what I did while not disclosing proprietary information. 

<br/><header style="text-align: center">The Goal</header><br/>

Through gene therapy, CODA Biotherapeutics has developed a chemogenetic platform designed to modulate neuronal activity with a tunable ligand-gated ion channel. These ligand-gated ion channels are engineered to be highly responsive to a specific proprietary small molecules and inactive otherwise. The interaction of the small molecule and engineered receptor allows for exquisite, dose-dependent control of the neurons to generate the therapeutic effect. The goal of the ML team is to predict functional properties of unobserved receptor mutants from measured properties of synthesized receptors in order to make recommendations to biology team of fruitful directions for future receptor synthesis.

<br/><br/><header style="text-align: center">The Data</header>

Using a set of biological assays, a dataset is collected consisting of a specific ligand-gated ion channel (protein), a particular ligand at a known concentration (drug) and the fluorescence inside of a cell indicating how sensitive the protein is to the drug at the given concentration. Bootstrapping is used to compute many logistic functions of fluorescence vs concentration in order to find a distribution of "half-maximal effective concentration" (EC50) values. Samples from the EC50 distribution become the output of our model, what the model attempts to predict for unseen protein-ligand pairs. The model's input consists of embedding representations of the drug molecular features and protein amino acid (AA) sequence. We attempt to leverage transfer learning by using embeddings that were trained on large databases of proteins and supplement protein embedding representation with physiochemical signatures for particular AA residues of interest. Drug / ligand embeddings represent whether common molecular domains that will interact with regions of the ion channel are present or absent.

<p align="center" width="100%">
  <figure style="width: 100%;">
    <img src="/images/CODA_ec50_posteriors.png" alt="Image" style="width: 70%;">
    <figcaption><strong>fig. 1</strong> EC50 posterior estimation examples for 4 receptor-ligand (RL) pairs. Each row represents an RL-pair. In each row, left panel: Blue dots show plate-reader input data points, ligand concentration vs. observed fluorescence measurements. By bootstrapping, multiple logistic curves can be fit to subsampling of data points, black lines. From each logistic curve, EC50 is concentration level (x-axis) at which curve predicts 50% quench. Right panels show probability density function of EC50 values across bootstrapping iterations.</figcaption>
  </figure>
</p>

<br/><br/><header style="text-align: center">Model Training</header><br/>

We used a version of a Multi-Layer Perceptron (MLP) model to predict EC50 values from receptor and ligand embeddings. We trained single models sweeping over hyperparameter values and choosing the parameter values that minimized the validation loss between model predicted EC50 and value extracted from EC50 posterior distribution constructed from lab measurements. For this process we used a Bayesian sweep over model hyperparameters from <a href="https://wandb.ai/site">Weights and Biases</a>. Once settled upon hyperparameter values, we fix them and train an ensemble model to make a distribution of predictions for EC50 value for a particular protein-ligand pair. 

<br/><br/><header style="text-align: center">Receptor Simulation & Evaluation</header>

<p align="center" width="100%">
  <figure style="width: 100%;">
    <img src="/images/CODA_ec50_ligand_and_ach2.png" alt="Image" style="width: 80%;">
    <figcaption><strong>fig. 2</strong> EC50 distributions of exogenous ligand in blue and endogenous molecule in yellow for a few unnamed mutated receptors. Ligand concentration on x-axis. Smaller concentrations and more leftward EC50 values indicate receptor sensitivity to ligand binding. Benchmark value in solid vertical line indicates median EC50 distribution for parental response to ligand. Target value in dashed vertical line is chosen by CODA researchers based on biological factors.</figcaption>
  </figure>
</p>

The model does not predict which receptor will have good characteristics; rather it predicts the distribution of EC50 values for a given receptor. Suggesting a new simulated protein mutation is done by “breeding” mutated proteins made in the lab for which we have EC50 measurements, selecting for the mutations that yield high fitness. The fitness of a receptor-ligand pair is determined by this combination of factors: (1). how responsive the receptor is to that ligand (smaller EC50 values in fig 2) and (2). how relatively unresponsive the receptor is to endogenous molecules (larger EC50 values in fig 2). 

At a conceptual level, the process of simulating receptors can be thought of in the following way: Each mutated protein is viewed as a bag-of-mutations, with each mutation consisting of a single amino acid change from the parental backbone. First, a pair of lab-made mutated proteins are drawn by weighted sampling based on their realtive fitness score. Then, the mutations from the pair are mixed together by randomly grabbing individual AA mutations, with a cap on maximum number of mutations allowed in a simulated protein. The novel receptor is fed through the model and a distribution of EC50 predictions are made. Simulated receptors, sorted by their fitness scores for a particular ligand, are delivered to biology team as recommendations for further exploration.


<br/><br/><header style="text-align: center">Model Evaluation</header><br/>

A major thrust of my work on this project has been replacing the existing protein embedding with a LLM embedding to take advantage of transfer learning from large language model trained on a large corpus of proteins. That replacement requires a head-to-head comparison of model performance with both embeddings. This comparison, illustrated in figure 3, is done for each RL-pair in the data set for three models.

<p align="center" width="100%">
  <figure style="width: 100%;">
    <img src="/images/CODA_model_compare.png" alt="Image" style="width: 100%;">
    <figcaption><strong>fig. 3</strong> Left side: For each protein-compound pair for which we have measured posterior EC50 values (red), we train an ensemble model to predict a distribution of EC50 values (blue). These are shown in the top plot. Similar to a Kolmogorov-Smirnov test, we use the cumulative distribution functions (CDF) of these two distributions to assess their similarity. CDFs are shown in bottom plot with green dashed line indicating the absolute value their difference. The metric we compute for a model/RL-pair combination is the mean value of the green curve, mean cs diff. Right side: Distribution of mean cs diff metric across 5462 RL-pairs for each of 3 models compared. Smaller values indicate more tightly fitting distributions for measured EC50 values and model predictions.</figcaption>
  </figure>
</p>

We found that the model trained using 100 dimensional original embeddings and the model trained using 1024 dimensional NLP embeddings performed comparably (green and red curves respectively in fig 3 right panel). When we used PCA to take only the top 100 dimensions of the NLP embedding, model performance declined significantly (blue curve in fig 3, right). Figure 4 shows some sample model predictions alongside measured EC50 distributions for the 3 models compared.

<p align="center" width="100%">
  <figure style="width: 100%;">
    <img src="/images/CODA_meas_n_pred.png" alt="Image" style="width: 100%;">
    <figcaption><strong>fig. 4</strong> Example RL-pairs where model prediction (smoother, more transparent distribution) well matches measured EC50 posterior (tighter, more opaque histogram). Each panel shows different model and ligand. All panels show endogenous molecule in yellow. Receptors are ordered by mean cs diff, see fig 3, for ligand and endogenous molecule from top to bottom, labeled in each plot by “fit”.</figcaption>
  </figure>
</p>


<br/><header style="text-align: center">Implementation & Deployment</header>

<p align="center" width="100%">
  <figure style="width: 100%;">
    <img src="/images/CODA_Airflow_Dags_flowchart.pdf" alt="Image" style="width: 100%;">
    <figcaption><strong>fig. 5</strong> Visual depiction of 5 Docker containers and 4 Airflow subdags that make up the coda discovery receptor prediction pipeline as of Feb 2022.</figcaption>
  </figure>
</p>

A second significant component of my effort on this project was to deploy the entire analysis pipeline to the AWS Cloud Computing architecture. Figure 5 below depicts a flowchart of the pipeline that was implemented using Airflow DAGs, Docker containers, AWS S3 and PostgreSQL databases and AWS EC2 compute nodes. 

I learned a lot from this experience, contributed significantly to the success of the project and added many tools to my toolkit, which I carry forward.

<br/><br/><header style="text-align: center">Summary</header><br/>

In summary, our main accomplishments in this project are:
  <ol>
    <li>embedding protein and ligand information as inputs</li>
    <li>encoding laboratory measurements as outputs</li>
    <li>training models including hyperparameter tuning</li>
    <li>simulating novel receptors - making predictions on them and evaluating fitness of those based on model predictions</li>
    <li>streamlining and automating the pipeline to run on AWS using databases, Airflow and Docker containers </li>
  </ol>
