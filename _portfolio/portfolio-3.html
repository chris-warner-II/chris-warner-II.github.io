---
title: "3. Protein-Drug Interaction Prediction using Machine Learning"
excerpt: "<img src='/images/CODA_Airflow_Dags_flowchart.pdf' width='550' height='400'><img src='/images/CODA_ec50_ligand_and_ach2.png' width='550' height='400'><br/> As a Machine Learning consultant for CODA Biotherapeutics, I contributed to an algorithm that guides development of a chemogenetic platform designed to modulate neuronal activity to therapeutic effect. The model predicts the fitness of novel synthetic ligand-gated ion channels that were engineered to be highly responsive to a specific small molecules but are otherwise inactive. Models were trained on proteins that had been synthesized and measured in the lab, a slow and expensive process. We used machine learning prediction on novel protein-ligand combinations to decrease the time and cost of this discovery process. We developed and deployed the pipeline to (1). embed protein and ligand information as inputs, (2). encode laboratory measurements as outputs, (3). train models including hyperparameter tuning, and (4). simulate novel receptors - making predictions on them and evaluating fitness of those based on model predictions."
collection: portfolio
---

<p style="text-align: center; font-size:11pt"><strong>Skills & Expertise:</strong> AWS Cloud Computing, </p>


<br/><header style="text-align: center">The Goal</header><br/>

CODA Biotherapeutics develops a chemogenetic platform designed to modulate neuronal activity. With chemogenetics, target neuronal populations are modified using gene therapy to express a tunable ligand-gated ion channel. These ligand-gated ion channels are engineered to be highly responsive to a specific proprietary small molecules but are otherwise inactive. The interaction of the small molecule and engineered receptor allows for exquisite, dose-dependent control of the neurons to generate the therapeutic effect. The goal of the ML team within CODA is to predict functional properties of unobserved receptor mutants from measured properties of synthesized receptors and to make recommendations to biology team for fruitful directions for future receptor synthesis.

<br/><br/><header style="text-align: center">The Data</header><br/>

<p align="center" width="100%">
  <figure style="width: 100%;">
    <img src="/images/CODA_ec50_posteriors.png" alt="Image" style="width: 70%;">
    <figcaption><strong>fig. 1</strong> EC50 posterior estimation examples for 4 receptor-ligand (RL) pairs. Each row represents an RL-pair. In each row, left panel: Blue dots show plate-reader input data points, ligand concentration vs. observed fluorescence measurements. By bootstrapping, multiple logistic curves can be fit to subsampling of data points, black lines. From each logistic curve, EC50 is concentration level (x-axis) at which curve predicts 50% quench. Right panels show probability density function of EC50 values across bootstrapping iterations.</figcaption>
  </figure>
</p>

Using a set of biological assays, a dataset is collected consisting of a specific ligand-gated ion channel (protein), a particular ligand at a known concentration (drug) and the fluorescence inside of a cell indicating how sensitive the protein was to the drug at the given concentration. We use subsets of that data to compute logistic functions of fluorescence vs concentration and can find a distribution of "half-maximal effective concentration" (EC50) values. Samples from the EC50 distribution become the output  of our model. 

The model's input consists of embedding representations of the drug molecular features and protein amino acid (AA) sequence. With the protein, we try to leverage transfer learning by using embeddings that were trained on large databases of proteins such as Prottrans and Protvec. We also supplement the protein embedding representation with physiochemical signatures of particular AA residues of interest. Drug / ligand embeddings are constructed from extended connectivity fingerprints which determine whether common molecular domains are present or absent.

<br/><br/><header style="text-align: center">Model Training</header><br/>

We built and trained a Multi-Layer Perceptron (MLP) model using 85% of the data for training and 15% for validation. We trained single models sweeping over hyperparameter values and choosing the pair that minimized the validation loss between model predicted EC50 and value extracted from EC50 posterior distribution constructed from lab measurements. For this process we used a Bayesian sweep over model hyperparameters from <a href="https://wandb.ai/site">Weights and Biases</a>. Once settled upon hyperparameter values, we fixed them and trained an ensemble model to make a distribution of predictions for EC50 value for a particular protein-ligand pair. 

<br/><br/><header style="text-align: center">Receptor Simulation & Evaluation</header><br/>

<p align="center" width="100%">
  <figure style="width: 100%;">
    <img src="/images/CODA_ec50_ligand_and_ach2.png" alt="Image" style="width: 100%;">
    <figcaption><strong>fig. 2</strong> EC50 distributions of exogenous ligand in blue and endogenous molecule in yellow for a few unnamed mutated receptors. Ligand concentration on x-axis. Smaller concentrations and more leftward EC50 values indicate receptor sensitivity to ligand binding. Benchmark value in solid vertical line indicates median EC50 distribution for parental response to ligand. Target value in dashed vertical line is chosen by CODA researchers based on biological factors.</figcaption>
  </figure>
</p>

The model does not predict which receptor will have good characteristics; rather it predicts the distribution of EC50 values for a given receptor. Suggesting a new simulated protein mutation is done by “breeding” mutated proteins made in the lab for which we have EC50 measurements, selecting for the mutations that yield high fitness. The fitness of a receptor-ligand pair is determined by this combination of factors: (1). how responsive the receptor is to that ligand (smaller ec50 distribution in fig 2) and (2). how relatively unresponsive the receptor is to endogenous molecules (larger ec50 distribution in fig 2). At a conceptual level, the process of simulating receptors can be thought of in the following way: Each mutated protein is viewed as a bag-of-mutations, with each mutation consisting of a single amino acid change from the parental backbone. First, a pair of lab-made mutated proteins are drawn by weighted sampling based on their realtive fitness score. Then, the mutations from the pair are mixed together by randomly grabbing individual AA mutations, with a cap on maximum number of mutations allowed in a simulated protein. The novel receptor is fed through the model and a distribution of EC50 predictions are made. Simulated receptors sorted by their fitness scores for a particular ligand are delivered to biology team as recommendations for further exploration.


<br/><br/><header style="text-align: center">Model Evaluation</header><br/>

A major thrust of my work on this project has been replacing the Protvecs protein embedding with a ProtBert embedding (referred to here as Prottrans). That replacement requires a head-to-head comparison of model performance with both embeddings. To get a picture of how accurately an ensemble model represents the entire EC50 posterior distribution for an RL-pair, we use the model fit with optimal parameters to predict an distribution of EC50 values and quantify the overlap of the model predicted EC50 distribution with the posterior distribution fit to measured EC50 values. This is done for each RL-pair in the data set. We performed this model comparison for three models: Protvecs, Prottrans and Prottrans-PCA100.

<p align="center" width="100%">
  <figure style="width: 100%;">
    <img src="/images/CODA_model_compare.png" alt="Image" style="width: 100%;">
    <figcaption><strong>fig. 3</strong> Left side: For each protein-compound pair for which we have measured posterior EC50 values (red), we train an ensemble model to predict a distribution of EC50 values (blue). These are shown in the top plot. Similar to a KolmogorovSmirnov test, we use the CDF of these two distributions to assess their similarity. CDFs are shown in bottom plot with green dashed line indicating the absolute value their difference. The metric we compute for a model/RL-pair combination is the mean value of the green curve, mean cs diff. Right side: Distribution of mean cs diff metric across 5462 RL-pairs for each of 3 models compared. Smaller values indicate more tightly fitting distributions for measured EC50 values and model predictions.</figcaption>
  </figure>
</p>

We found that the model trained using 100 dimensional Protvec embeddings and the model trained using 1024 dimensional Prottrans embeddings performed comparably. When we used PCA to take only the top 100 dimensions of the Prottrans embedding, model performance declined significantly.

<p align="center" width="100%">
  <figure style="width: 100%;">
    <img src="/images/CODA_meas_n_pred.png" alt="Image" style="width: 100%;">
    <figcaption><strong>fig. 4</strong> Example RL-pairs where model prediction (smoother, more transparent distribution) well matches measured EC50 posterior (tighter, more opaque histogram). Each panel shows different model and ligand. All panels show endogenous molecule in yellow. Receptors are ordered by mean cs diff, see fig 3, for ligand and endogenous molecule from top to bottom, labeled in each plot by “fit”.</figcaption>
  </figure>
</p>

A second significant component of my effort on this project was to deploy the entire analysis pipeline to AWS. Figure 5 below depicts a flowchart of the pipeline that was implemented using Airflow DAGs, Docker containers, AWS S3 and PostgreSQL databases and AWS EC2 compute nodes. 

<br/><br/><header style="text-align: center">Implementation & Deployment</header><br/>

<p align="center" width="100%">
  <figure style="width: 100%;">
    <img src="/images/CODA_Airflow_Dags_flowchart.pdf" alt="Image" style="width: 100%;">
    <figcaption><strong>fig. 5</strong> Visual depiction of 5 docker containers and 4 airflow subdags that make up the coda discovery receptor prediction pipeline as of Feb 2022.</figcaption>
  </figure>
</p>

<br/><br/><header style="text-align: center">Summary</header><br/>

In this project, the big take-aways are:
  <ol>
    <li>one</li>
    <li>one</li>
    <li>one</li>
    <li>one</li>
    <li>one</li>
    <li>one</li>
    <li>one</li>
  </ol>

