---
title: "1. Predicting Protein-Drug Interaction using Machine Learning (Professional Work)"
excerpt: "<img src='/images/CODA_Airflow_Dags_flowchart.pdf' width='500' height='400'><img src='/images/CODA_ec50_ligand_and_ach2.png' width='400' height='350'><br/> As a Machine Learning consultant for CODA Biotherapeutics, I contributed to an algorithm that guides development of a chemogenetic platform designed to modulate neuronal activity to therapeutic effect. The model predicts the fitness of novel synthetic ligand-gated ion channels engineered to be highly and selectively responsive to specific small molecules but otherwise inactive. Models are trained on data from proteins synthesized and measured in the lab, a slow and expensive process. We used machine learning to predict on novel protein-ligand combinations to decrease the time and cost of the discovery process."
collection: portfolio
---

<p style="text-align: center; font-size:11pt"><strong>Skills & Expertise:</strong> Bioinformatics, Multi-layer Perceptron, Keras, LLM Embeddings, AWS Cloud Computing, Weights & Biases, Docker containers, Python </p>

<br/><header style="text-align: center">The Goal</header><br/>

Through gene therapy, CODA Biotherapeutics has developed a chemogenetic platform designed to modulate neuronal activity with a tunable ligand-gated ion channel. These ligand-gated ion channels are engineered to be highly responsive to a specific proprietary small molecules but are otherwise inactive. The interaction of the small molecule and engineered receptor allows for exquisite, dose-dependent control of the neurons to generate the therapeutic effect. The goal of the ML team within CODA is to predict functional properties of unobserved receptor mutants from measured properties of synthesized receptors and to make recommendations to biology team for fruitful directions for future receptor synthesis.

<br/><br/><header style="text-align: center">The Data</header>

<p align="center" width="100%">
  <figure style="width: 100%;">
    <img src="/images/CODA_ec50_posteriors.png" alt="Image" style="width: 70%;">
    <figcaption><strong>fig. 1</strong> EC50 posterior estimation examples for 4 receptor-ligand (RL) pairs. Each row represents an RL-pair. In each row, left panel: Blue dots show plate-reader input data points, ligand concentration vs. observed fluorescence measurements. By bootstrapping, multiple logistic curves can be fit to subsampling of data points, black lines. From each logistic curve, EC50 is concentration level (x-axis) at which curve predicts 50% quench. Right panels show probability density function of EC50 values across bootstrapping iterations.</figcaption>
  </figure>
</p>

Using a set of biological assays, a dataset is collected consisting of a specific ligand-gated ion channel (protein), a particular ligand at a known concentration (drug) and the fluorescence inside of a cell indicating how sensitive the protein is to the drug at the given concentration. We use bootstrapping to compute logistic functions of fluorescence vs concentration and can find a distribution of "half-maximal effective concentration" (EC50) values. Samples from the EC50 distribution become the output of our model, what the model attempts to predict. The model's input consists of embedding representations of the drug molecular features and protein amino acid (AA) sequence. With the protein, we attempt to leverage transfer learning by using embeddings that were trained on large databases of proteins and supplement protein embedding representation with physiochemical signatures for particular AA residues of interest. Drug / ligand embeddings are constructed from extended connectivity fingerprints which determine whether common molecular domains are present or absent.

<br/><br/><header style="text-align: center">Model Training</header><br/>

We build and train a Multi-Layer Perceptron (MLP) model using 85% of the data for training and 15% for validation. We trained single models sweeping over hyperparameter values and choosing the pair that minimized the validation loss between model predicted EC50 and value extracted from EC50 posterior distribution constructed from lab measurements. For this process we used a Bayesian sweep over model hyperparameters from <a href="https://wandb.ai/site">Weights and Biases</a>. Once settled upon hyperparameter values, we fix them and train an ensemble model to make a distribution of predictions for EC50 value for a particular protein-ligand pair. 

<br/><br/><header style="text-align: center">Receptor Simulation & Evaluation</header>

<p align="center" width="100%">
  <figure style="width: 100%;">
    <img src="/images/CODA_ec50_ligand_and_ach2.png" alt="Image" style="width: 80%;">
    <figcaption><strong>fig. 2</strong> EC50 distributions of exogenous ligand in blue and endogenous molecule in yellow for a few unnamed mutated receptors. Ligand concentration on x-axis. Smaller concentrations and more leftward EC50 values indicate receptor sensitivity to ligand binding. Benchmark value in solid vertical line indicates median EC50 distribution for parental response to ligand. Target value in dashed vertical line is chosen by CODA researchers based on biological factors.</figcaption>
  </figure>
</p>

The model does not predict which receptor will have good characteristics; rather it predicts the distribution of EC50 values for a given receptor. Suggesting a new simulated protein mutation is done by “breeding” mutated proteins made in the lab for which we have EC50 measurements, selecting for the mutations that yield high fitness. The fitness of a receptor-ligand pair is determined by this combination of factors: (1). how responsive the receptor is to that ligand (smaller EC50 values in fig 2) and (2). how relatively unresponsive the receptor is to endogenous molecules (larger EC50 values in fig 2). 

At a conceptual level, the process of simulating receptors can be thought of in the following way: Each mutated protein is viewed as a bag-of-mutations, with each mutation consisting of a single amino acid change from the parental backbone. First, a pair of lab-made mutated proteins are drawn by weighted sampling based on their realtive fitness score. Then, the mutations from the pair are mixed together by randomly grabbing individual AA mutations, with a cap on maximum number of mutations allowed in a simulated protein. The novel receptor is fed through the model and a distribution of EC50 predictions are made. Simulated receptors sorted by their fitness scores for a particular ligand are delivered to biology team as recommendations for further exploration.


<br/><br/><header style="text-align: center">Model Evaluation</header><br/>

A major thrust of my work on this project has been replacing the Protvecs protein embedding with a Prottrans embedding to take advantage of transfer learning from large language model trained on a large corpus of proteins. That replacement requires a head-to-head comparison of model performance with both embeddings. This comparison, illustrated in figure 3, is done for each RL-pair in the data set for three models: Protvecs, Prottrans and Prottrans-PCA100.

<p align="center" width="100%">
  <figure style="width: 100%;">
    <img src="/images/CODA_model_compare.png" alt="Image" style="width: 100%;">
    <figcaption><strong>fig. 3</strong> Left side: For each protein-compound pair for which we have measured posterior EC50 values (red), we train an ensemble model to predict a distribution of EC50 values (blue). These are shown in the top plot. Similar to a KolmogorovSmirnov test, we use the CDF of these two distributions to assess their similarity. CDFs are shown in bottom plot with green dashed line indicating the absolute value their difference. The metric we compute for a model/RL-pair combination is the mean value of the green curve, mean cs diff. Right side: Distribution of mean cs diff metric across 5462 RL-pairs for each of 3 models compared. Smaller values indicate more tightly fitting distributions for measured EC50 values and model predictions.</figcaption>
  </figure>
</p>

We found that the model trained using 100 dimensional Protvec embeddings and the model trained using 1024 dimensional Prottrans embeddings performed comparably (green and red curves respectively in fig 3 right panel). When we used PCA to take only the top 100 dimensions of the Prottrans embedding, model performance declined significantly (blue curve in fig 3, right). Figure 4 shows some sample model predictions alongside measured EC50 distributions for the 3 models compared.

<p align="center" width="100%">
  <figure style="width: 100%;">
    <img src="/images/CODA_meas_n_pred.png" alt="Image" style="width: 100%;">
    <figcaption><strong>fig. 4</strong> Example RL-pairs where model prediction (smoother, more transparent distribution) well matches measured EC50 posterior (tighter, more opaque histogram). Each panel shows different model and ligand. All panels show endogenous molecule in yellow. Receptors are ordered by mean cs diff, see fig 3, for ligand and endogenous molecule from top to bottom, labeled in each plot by “fit”.</figcaption>
  </figure>
</p>


<br/><header style="text-align: center">Implementation & Deployment</header>

<p align="center" width="100%">
  <figure style="width: 100%;">
    <img src="/images/CODA_Airflow_Dags_flowchart.pdf" alt="Image" style="width: 100%;">
    <figcaption><strong>fig. 5</strong> Visual depiction of 5 docker containers and 4 airflow subdags that make up the coda discovery receptor prediction pipeline as of Feb 2022.</figcaption>
  </figure>
</p>

A second significant component of my effort on this project was to deploy the entire analysis pipeline to the AWS Cloud Computing architecture. Figure 5 below depicts a flowchart of the pipeline that was implemented using Airflow DAGs, Docker containers, AWS S3 and PostgreSQL databases and AWS EC2 compute nodes. 

Unfortunately, since I handed off the project, CODA Biotherapeutics has gone <a href="https://www.biospace.com/article/in-tough-market-bay-area-biotech-coda-shutters/">out of business</a>. I did learn a lot from this experience and added many tools to my toolkit, which I carry forward.

<br/><br/><header style="text-align: center">Summary</header><br/>

In summary, our main accomplishments in this project are:
  <ol>
    <li>embedding protein and ligand information as inputs</li>
    <li>encoding laboratory measurements as outputs</li>
    <li>training models including hyperparameter tuning</li>
    <li>simulating novel receptors - making predictions on them and evaluating fitness of those based on model predictions</li>
    <li>streamlining and automating the pipeline to run on AWS using databases, Airflow and Docker containers </li>
  </ol>
